{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm        # Progress bar library for Jupyter Notebook\n",
    "\n",
    "# Deep learning framework for building and training models\n",
    "import tensorflow as tf\n",
    "## Pre-trained model for image feature extraction\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "## Tokenizer class for captions tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "## Function for padding sequences to a specific length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## Class for defining Keras models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate, Bidirectional, Dot, Activation, RepeatVector, Multiply, Lambda, BatchNormalization\n",
    "\n",
    "# For checking score\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the input and output directory\n",
    "INPUT_DIR = './data/'\n",
    "OUTPUT_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use pretraind vgg model\n",
    "# Load the vgg16 model\n",
    "model = VGG16()\n",
    "\n",
    "# Restructuring the model to remove the last classification layer, this will give us access to the output features of the model\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# Printing the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vgg16 model\n",
    "model = VGG16()\n",
    "\n",
    "# Restructuring the model to remove the last classification layer, this will give us access to the output features of the model\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# Printing the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store image features\n",
    "image_features = {}\n",
    "\n",
    "# Define the directory path where images are located\n",
    "img_dir = os.path.join(INPUT_DIR, 'Images')\n",
    "\n",
    "# Loop through each image in the directory\n",
    "for img_name in tqdm(os.listdir(img_dir)):\n",
    "    # Load the image from file\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    # Convert image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # Reshape the data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # Preprocess the image for ResNet50\n",
    "    image = preprocess_input(image)\n",
    "    # Extract features using the pre-trained ResNet50 model\n",
    "    image_feature = model.predict(image, verbose=0)\n",
    "    # Get the image ID by removing the file extension\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # Store the extracted feature in the dictionary with the image ID as the key\n",
    "    image_features[image_id] = image_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the image features in pickle\n",
    "pickle.dump(image_features, open(os.path.join(OUTPUT_DIR, 'img_features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from pickle file\n",
    "pickle_file_path = os.path.join(OUTPUT_DIR, 'img_features.pkl')\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    loaded_features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(INPUT_DIR, 'captions.txt'), 'r') as file:\n",
    "    next(file)\n",
    "    captions_doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping of image to captions\n",
    "image_to_captions_mapping = defaultdict(list)\n",
    "\n",
    "# Process lines from captions_doc\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # Split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "    image_id, *captions = tokens\n",
    "    # Remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # Convert captions list to string\n",
    "    caption = \" \".join(captions)\n",
    "    # Store the caption using defaultdict\n",
    "    image_to_captions_mapping[image_id].append(caption)\n",
    "\n",
    "# Print the total number of captions\n",
    "total_captions = sum(len(captions) for captions in image_to_captions_mapping.values())\n",
    "print(\"Total number of captions:\", total_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for processing the captions\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # Take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # Preprocessing steps\n",
    "            # Convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # Remove non-alphabetical characters\n",
    "            caption = ''.join(char for char in caption if char.isalpha() or char.isspace())\n",
    "            # Remove extra spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # Add unique start and end tokens to the caption\n",
    "            caption = 'startseq ' + ' '.join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean(image_to_captions_mapping)\n",
    "# Creating a List of All Captions\n",
    "all_captions = [caption for captions in image_to_captions_mapping.values() for caption in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate maximum caption length\n",
    "max_caption_length = max(len(tokenizer.texts_to_sequences([caption])[0]) for caption in all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\"Maximum Caption Length:\", max_caption_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a List of Image IDs\n",
    "image_ids = list(image_to_captions_mapping.keys())\n",
    "# Splitting into Training and Test Sets\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator function\n",
    "def data_generator(data_keys, image_to_captions_mapping, features, tokenizer, max_caption_length, vocab_size, batch_size):\n",
    "    # Lists to store batch data\n",
    "    X1_batch, X2_batch, y_batch = [], [], []\n",
    "    # Counter for the current batch size\n",
    "    batch_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Loop through each image in the current batch\n",
    "        for image_id in data_keys: \n",
    "            # Get the captions associated with the current image\n",
    "            captions = image_to_captions_mapping[image_id]\n",
    "\n",
    "            # Loop through each caption for the current image\n",
    "            for caption in captions:\n",
    "                # Convert the caption to a sequence of token IDs\n",
    "                caption_seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                # Loop through the tokens in the caption sequence\n",
    "                for i in range(1, len(caption_seq)):\n",
    "                    # Split the sequence into input and output pairs\n",
    "                    in_seq, out_seq = caption_seq[:i], caption_seq[i]\n",
    "\n",
    "                    # Pad the input sequence to the specified maximum caption length\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]\n",
    "\n",
    "                    # Convert the output sequence to one-hot encoded format\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    # Append data to batch lists\n",
    "                    X1_batch.append(features[image_id][0])  # Image features\n",
    "                    X2_batch.append(in_seq)  # Input sequence\n",
    "                    y_batch.append(out_seq)  # Output sequence\n",
    "\n",
    "                    # Increase the batch counter\n",
    "                    batch_count += 1\n",
    "\n",
    "                    # If the batch is complete, yield the batch and reset lists and counter\n",
    "                    if batch_count == batch_size:\n",
    "                        X1_batch, X2_batch, y_batch = np.array(X1_batch), np.array(X2_batch), np.array(y_batch)\n",
    "                        yield [X1_batch, X2_batch], y_batch\n",
    "                        X1_batch, X2_batch, y_batch = [], [], []\n",
    "                        batch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "fe2_projected = RepeatVector(max_caption_length)(fe2)\n",
    "fe2_projected = Bidirectional(LSTM(256, return_sequences=True))(fe2_projected)\n",
    "\n",
    "# Sequence feature layers\n",
    "inputs2 = Input(shape=(max_caption_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = Bidirectional(LSTM(256, return_sequences=True))(se2)\n",
    "\n",
    "# Apply attention mechanism using Dot product\n",
    "attention = Dot(axes=[2, 2])([fe2_projected, se3])  # Calculate attention scores\n",
    "\n",
    "# Softmax attention scores\n",
    "attention_scores = Activation('softmax')(attention)\n",
    "\n",
    "# Apply attention scores to sequence embeddings\n",
    "attention_context = Lambda(lambda x: tf.einsum('ijk,ijl->ikl', x[0], x[1]))([attention_scores, se3])\n",
    "\n",
    "# Sum the attended sequence embeddings along the time axis\n",
    "context_vector = tf.reduce_sum(attention_context, axis=1)\n",
    "\n",
    "# Decoder model\n",
    "decoder_input = concatenate([context_vector, fe2], axis=-1)\n",
    "decoder1 = Dense(256, activation='relu')(decoder_input)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder1)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Visualize the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs, batch size\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate the steps_per_epoch based on the number of batches in one epoch\n",
    "steps_per_epoch = ceil(len(train) / batch_size)\n",
    "validation_steps = ceil(len(test) / batch_size)  # Calculate the steps for validation data\n",
    "\n",
    "# Loop through the epochs for training\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Set up data generators\n",
    "    train_generator = data_generator(train, image_to_captions_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)\n",
    "    test_generator = data_generator(test, image_to_captions_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)\n",
    "    \n",
    "    model.fit(train_generator, epochs=1, steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=test_generator, validation_steps=validation_steps,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('./mymodel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
